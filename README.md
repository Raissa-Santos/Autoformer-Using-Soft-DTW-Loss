# **Autoformer com Soft-DTW para Predi√ß√£o em S√©ries Temporais de Criptomoedas** üìäüí∏

## **üìñ Descri√ß√£o**

Neste trabalho, prop√µe-se a adapta√ß√£o do modelo Autoformer para incorporar o Soft-DTW Loss.

### **Objetivos:**
- Adaptar o Autoformer para utilizar o Soft-DTW como medida de perda durante o treinamento.
- Comparar o desempenho do modelo adaptado com o original e outros modelos baseline.  

---

## **üöÄ Funcionalidades**

- ‚úÖ Predi√ß√£o com Long Short Term Memory (LSTM).
- ‚úÖ Predi√ß√£o com Gated Recurrent Unit (GRU).
- ‚úÖ Predi√ß√£o com Autoformer Simplficado utilizando MSE Loss.
- ‚úÖ Predi√ß√£o com Autoformer Simplficado utilizando Soft-DTW Loss.
- ‚úÖ Aplica√ß√£o de m√©tricas de desempenho para compara√ß√£o dos modelos.

---

## **üõ†Ô∏è Tecnologias Utilizadas**

Este projeto foi desenvolvido com as seguintes bibliotecas e ferramentas:

- **Python**: Linguagem principal utilizada para o desenvolvimento do projeto.
- **Pandas**: Manipula√ß√£o e an√°lise de dados estruturados em DataFrames.
- **NumPy**: Opera√ß√µes matem√°ticas eficientes em arrays multidimensionais.
- **Matplotlib**: Cria√ß√£o de gr√°ficos e visualiza√ß√µes simples.
- **Seaborn**: Visualiza√ß√£o de dados estat√≠sticos com gr√°ficos detalhados.
- **Statsmodels**: Decomposi√ß√£o sazonal e an√°lise de s√©ries temporais.
- **PyTorch**: Framework para constru√ß√£o e treinamento de modelos de aprendizado profundo.
- **torch.nn**: M√≥dulo de constru√ß√£o de redes neurais com camadas personalizadas.
- **Scikit-learn**: Utilizado para divis√£o de dados com `train_test_split`.
- **tslearn.metrics**: Implementa√ß√£o da fun√ß√£o de perda DTW suave (`SoftDTWLossPyTorch`).
- **MultiheadAttention (PyTorch)**: Mecanismo de aten√ß√£o multi-cabe√ßa para modelos baseados em Transformer.
- **Scikit-learn (M√©tricas)**: Avalia√ß√£o do modelo com:
  - `mean_absolute_error` (Erro Absoluto M√©dio)
  - `mean_squared_error` (Erro Quadr√°tico M√©dio)
  - `root_mean_squared_error` (Raiz do Erro Quadr√°tico M√©dio)

